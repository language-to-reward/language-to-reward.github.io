
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Language to Rewards for Robotic Skill Synthesis</title>

    <meta name="description" content="Language to Rewards for Robotic Skill Synthesis">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Language to Rewards for Robotic Skill Synthesis" />
    <meta name="twitter:description" content="Project page for Language to Rewards for Robotic Skill Synthesis." />
    <!-- <meta name="twitter:image" content="https://language-to-reward.github.io/img/rt1-teaser.jpeg" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css"> -->
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script>hljs.highlightAll();</script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+4">Language to Rewards for Robotic Skill Synthesis</font></strong>  
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Wenhao Yu</li> <li>Nimrod Gileadi</li> <li>Chuyuan Fu</li> <li>Sean Kirmani</li> <li>Kuang-Huei Lee</li> <br>
                 <li>Montse Gonzalez Arenas</li> <li>Hao-Tien Lewis Chiang</li> <li>Tom Erez</li> <li>Leonard Hasenclever</li>  <br>
                 <li>Jan Humplik</li> <li>Brian Ichter</li> <li>Ted Xiao</li> <li>Peng Xu</li> <li>Andy Zeng</li> <li>Tingnan Zhang</li> <br>
                 <li>Nicolas Heess</li> <li>Dorsa Sadigh</li> <li>Jie Tan</li> <li>Yuval Tassa</li> <li>Fei Xia</li>  <br>
                <br>
		    <br><br>
                    <a href="https://www.deepmind.com/">
                    <image src="img/google-deepmind-logo.png" height="37px"> </a>
                    
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="assets/l2r.pdf">
                            <image src="img/paper_small2.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li>
                         <li>
                            <a href="https://github.com/google-research/robotics_transformer">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>                   
                        </li>  -->
                    </ul>
                </div>
        </div>


   
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
        	    	<!-- <video id="v0" width="100%" playsinline autoplay muted loop controls>
                       <source src="img/rt1_mosaic_comp.mp4" type="video/mp4">
                   </video> -->
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Large language models (LLMs) have demonstrated exciting progress in
acquiring diverse new capabilities through in-context learning, ranging from logical
reasoning to code-writing. Robotics researchers have also explored using LLMs to
advance the capabilities of robotic control. However, since low-level robot actions are
hardware-dependent and underrepresented in LLM training corpora, existing efforts in
applying LLMs to robotics have largely treated LLMs as semantic planners or relied
on human-engineered control primitives to interface with the robot. On the other hand,
reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable
to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this
realization by utilizing LLMs to define reward parameters that can be optimized and
accomplish variety of robotic tasks. Using reward as the intermediate interface generated
by LLMs, we can effectively bridge the gap between high-level language instructions
or corrections to low-level robot actions. Meanwhile, combining this with a real-time
optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where
users can immediately observe the results and provide feedback to the system. To
systematically evaluate the performance of our proposed method, we designed a total of
17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles 90% of the designed tasks, while a
baseline using primitive skills as the interface with Code-as-policies achieves 50% of the
tasks. We further validated our method on a real robot arm where complex manipulation
skills such as non-prehensile pushing emerge through our interactive system.
                </p>
            </div>
        </div>


	    <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

     
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Approach Overview
                </h3>
                <p style="text-align:center;">
                    <image src="img/l2r_overview.png" class="img-responsive">
                    </p>
                <p class="text-justify">
                    The recent rapid progress in Large Language Models (LLMs) has inspired notable developments in leveraging LLMs to drive robot behaviors:
                    from step-by-step planning, goal-oriented dialogue, to robot-code-writing agents. While these methods impart new modes of compositional
                    generalization, they focus on using language to concatenate together new behaviors from an existing library of control primitives that are 
                    either manually-engineered or learned a priori. On the other hand, leveraging LLMs to directly modulate low-level robot behavior still remains an open problem
                    due to that low-level robot actions are hardware-dependent and underrepresented in LLM training corpora.
                    <br>
                    In this work, we aim to develop an interactive system that leverages the power of LLMs to acquire low-level robotic skills 
                    Our proposed system consists of two key components: i) a Reward Translator, built uponpre-trained Large Language Models (LLMs) [10], 
                    that interacts with and understands user intents andmodulates all reward parametersÏˆand weightsw, and ii) a Motion Controller, 
                    based on MuJoCo MPC, that takes the generated reward and interactively optimize the optimal action sequence.
                </p>

            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reward Translator
                </h3>
            <p style="text-align:center;">
                <image src="img/reward_translator.png" class="img-responsive">
            </p>
                <p class="text-justify">
                    We build the Reward Translator based on LLMs to map user interactions to reward functions corresponding to the desired robot motion.
                    As reward tuning is highly domain-specific and requires expert knowledge, it is unsurprising that LLMs trained on generic language datasets
                    cannot directly generate a reward for a specific hardware. Instead, we explore the in-context learning ability of LLMs to achieve this goal.
                    <br>
                    More concretely, we decompose the problem of language to reward into two stages: motion description and reward coding. During motion description stage,
                    we design a prompt to instruct an LLM to interpret and expand the user input into a natural language description of the desired robot motion following a pre-defined template. 
                    While during the reward coding stage, we use another prompt to instruct the LLM to translate the motion description into reward specifying code.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motion Controller
                </h3>
         
                <p class="text-justify">
                The Motion Controller needs to map the reward function generated by the Reward Translatorto low-level robot actions that maximize the accumulated reward specified by the Reward Translator.
                There are a few possible ways to achieve this, including using reinforcement learning (RL), offline trajectory optimization, or, as in this work, receding horizon trajectory optimization, i.e., model predictive control (MPC). 
                Specifically, we use an open-source implementation based on the MuJoCo simulator, MJPC. 
                At each control step, MJPC plans a sequence of optimized actions and sends to the robot. The robot applies the action corresponding to its current timestamp, advances to the next step, and sends the updated robot states to the MJPC planner to initiate the next planning cycle. 
                The frequent re-planning in MPC empowers its robustness to uncertainties in the system and, importantly, enables interactive motion synthesis and correction. 
                    </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
         
                <p class="text-justify">
                    We evaluate our approach on two simulated robotic systems: a quadruped robot, and a dexterous robot manipulator.
                    We design a diverse set of tasks for each robot to demonstrate the capability of our proposed system. Some examples of the resulting
                    robot motions can be found below. 
                </p>
            </div>
        </div>

        <div class="row">  
            <div class="col-md-4">
            <p style="text-align:center;">
              <video width="100%" height="100%" controls autoplay loop muted>
                <source src="videos/sim/face_sunset.mp4" type="video/mp4">
              </video>
              <p class="text-justify">
                User instruction: It's late in the afternoon, make the robot head towards the sunset.
                <br>
                <code>
                import numpy as np

reset_reward()

set_torso_targets(0.3, np.deg2rad(0), np.deg2rad(0), None, (0.5, 0.0), np.pi, None)

set_feet_stepping_parameters('front_left', 2.0, 0.5, 0.0, 0.1, 0.1, True)
set_feet_stepping_parameters('back_left', 2.0, 0.5, 0.5, 0.1, 0.1, True)
set_feet_stepping_parameters('front_right', 2.0, 0.5, 0.5, 0.1, 0.1, True)
set_feet_stepping_parameters('back_right', 2.0, 0.5, 0.0, 0.1, 0.1, True)

execute_plan()
</code>
              </p>
            </p>
            </div>
            <div class="col-md-4">
              <p style="text-align:center;">
              <video width="100%" height="100%" controls autoplay loop muted>
                <source src="videos/sim/sit.mp4" type="video/mp4">
              </video>
              <p class="text-justify">
                User instruction: Sit like a dog.
                <code>
                    import numpy as np
                    
                    reset_reward()
                    set_torso_targets(0.15, np.deg2rad(45), np.deg2rad(0), (0.0, 0.0), None, 0, None)
                    
                    set_feet_pos_parameters('front_left', 0.0, None, None)
                    set_feet_pos_parameters('back_left', 0.0, None, None)
                    set_feet_pos_parameters('front_right', 0.0, None, None)
                    set_feet_pos_parameters('back_right', 0.0, None, None)
                    
                    execute_plan()
                </code>
              </p>
              </p>
            </div>   
            <div class="col-md-4">
              <p style="text-align:center;">
              <video width="100%" height="100%" controls autoplay loop muted>
                <source src="videos/sim/biped.mp4" type="video/mp4">
              </video>
              <p class="text-justify">
                User instruction: Stand up on two back feet like a human.
                <code>
import numpy as np

reset_reward()
set_torso_targets(0.65, np.deg2rad(90), np.deg2rad(0), (0.0, 0.0), None, 0, None)

set_feet_pos_parameters('front_left', 0.65, None, None)
set_feet_pos_parameters('back_left', 0.0, None, None)
set_feet_pos_parameters('front_right', 0.65, None, None)
set_feet_pos_parameters('back_right', 0.0, None, None)

execute_plan()
                </code>
              </p>
              </p>
            </div> 
            <div class="col-md-4">
                <p style="text-align:center;">
                <video width="100%" height="100%" controls autoplay loop muted>
                  <source src="videos/sim/bowl2.mp4" type="video/mp4">
                </video>
                <p class="text-justify">
                  User instruction: Flp the bowl.
                  <code>
import numpy as np

reset_reward()  # This is a new task so reset reward; otherwise we don't need it
set_l2_distance_reward("palm", "bowl")
set_obj_orientation_reward("bowl", np.deg2rad(180))

execute_plan(2)
                  </code>
                </p>
                </p>
            </div> 
            <div class="col-md-4">
                <p style="text-align:center;">
                <video width="100%" height="100%" controls autoplay loop muted>
                  <source src="videos/sim/open_faucet.mp4" type="video/mp4">
                </video>
                <p class="text-justify">
                  User instruction: Turn on the faucet.
                  <code>
import numpy as np

reset_reward() # This is a new task so reset reward; otherwise we don't need it
set_l2_distance_reward("palm", "faucet_handle")
set_joint_fraction_reward("faucet", 1) # Open the faucet

execute_plan(4)
                  </code>
                </p>
                </p>
            </div> 
            <div class="col-md-4">
                <p style="text-align:center;">
                <video width="100%" height="100%" controls autoplay loop muted>
                  <source src="videos/sim/upright_box.mp4" type="video/mp4">
                </video>
                <p class="text-justify">
                  User instruction: Make the box upright.
                  <code>
import numpy as np

reset_reward()  # This is a new task so reset reward; otherwise we don't need it
set_l2_distance_reward("palm", "box")
set_obj_orientation_reward("box", np.deg2rad(90))

execute_plan()
                  </code>
                </p>
                </p>
            </div> 
      </div>
         -->

      <div class="row" id="demo">
        <div class="col-md-8 col-md-offset-2 col-xs-12">
            <h3>
                Results
            </h3>
            <p>
                <p class="text-justify">
                    We evaluate our approach on two simulated robotic systems: a quadruped robot, and a dexterous robot manipulator.
                    We design a diverse set of tasks for each robot to demonstrate the capability of our proposed system. Some examples of the resulting
                    robot motions can be found below. 
                </p>
            </p>

            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-4">
                    <img src="videos/sim/bowl2.png" width="100%" alt=" User instruction: Flp the bowl. [sep]import numpy as np\\reset_reward()  # This is a new task so reset reward; otherwise we don't need it\\set_l2_distance_reward('palm', 'bowl')\\set_obj_orientation_reward('bowl', np.deg2rad(180))\\execute_plan(2)" onclick="populateDemo(this);">
                </div>
                <div class="col-md-4 col-sm-4 col-xs-4">
                    <img src="videos/sim/open_faucet.png" width="100%" alt="User instruction: Turn on the faucet.[sep]import numpy as np\\reset_reward() # This is a new task so reset reward; otherwise we don't need it\\set_l2_distance_reward('palm', 'faucet_handle')\\set_joint_fraction_reward('faucet', 1) # Open the faucet\\execute_plan(4)" onclick="populateDemo(this);">
                </div>
                <div class="col-md-4 col-sm-4 col-xs-4">
                    <img src="videos/sim/upright_box.png" width="100%" alt="User instruction: Make the box upright.[sep]import numpy as np\\reset_reward()  # This is a new task so reset reward; otherwise we don't need it\\set_l2_distance_reward('palm', 'box')\\set_obj_orientation_reward('box', np.deg2rad(90))\\execute_plan()" onclick="populateDemo(this);">
                </div>
            </div>
            <p></p>

            <div class="row border rounded" style="padding-top:10px; padding-bottom:10px;">
                <div class="col-md-6">
                    <!-- <img class="img img-responsive" id="expandedImg" style="height: 300px;" src="img/placeholder.png"> -->
                    <video width="100%" height="100%" id="demo-video" controls autoplay loop muted>
                        <source id="expandedImg" src="videos/sim/upright_box.mp4" type="video/mp4">
                        <!-- <img class="img img-responsive" id="expandedImg" style="height: 300px;" src="img/placeholder.png"> -->
                    </video>

                </div>
                <div class="col-md-6">
                <div id="imgtext">Prompt text in gray.</div>
                <div>
                    <pre><code class="language-python" id="answer">L2R response shown within code block.</code></pre>
                </div>

                </div>
                
            </div>
            <p></p>
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-4">
                    <img src="videos/sim/face_sunset.png" width="100%" alt=" User instruction: It's late in the afternoon, make the robot head towards the sunset.[sep]reset_reward()\\set_torso_targets(0.3, np.deg2rad(0), np.deg2rad(0), None, (0.5, 0.0), np.pi, None)\\\\set_feet_stepping_parameters('front_left', 2.0, 0.5, 0.0, 0.1, 0.1, True)\\set_feet_stepping_parameters('back_left', 2.0, 0.5, 0.5, 0.1, 0.1, True)\\set_feet_stepping_parameters('front_right', 2.0, 0.5, 0.5, 0.1, 0.1, True)\\set_feet_stepping_parameters('back_right', 2.0, 0.5, 0.0, 0.1, 0.1, True)\\\\execute_plan()" onclick="populateDemo(this);">
                </div>
                <div class="col-md-4 col-sm-4 col-xs-4">
                    <img src="videos/sim/biped.png" width="100%" alt="User instruction: Stand up on two back feet like a human.[sep]import numpy as np

reset_reward()
set_torso_targets(0.65, np.deg2rad(90), np.deg2rad(0), (0.0, 0.0), None, 0, None)

set_feet_pos_parameters('front_left', 0.65, None, None)
set_feet_pos_parameters('back_left', 0.0, None, None)
set_feet_pos_parameters('front_right', 0.65, None, None)
set_feet_pos_parameters('back_right', 0.0, None, None)

execute_plan()" onclick="populateDemo(this);">
                </div>
                <div class="col-md-4 col-sm-4 col-xs-4">
                    <img src="videos/sim/sit.png" width="100%" alt="User instruction: Sit like a dog.[sep]import numpy as np

reset_reward()
set_torso_targets(0.15, np.deg2rad(45), np.deg2rad(0), (0.0, 0.0), None, 0, None)

set_feet_pos_parameters('front_left', 0.0, None, None)
set_feet_pos_parameters('back_left', 0.0, None, None)
set_feet_pos_parameters('front_right', 0.0, None, None)
set_feet_pos_parameters('back_right', 0.0, None, None)

execute_plan()" onclick="populateDemo(this);">
                </div>
            </div>
            </div>
            

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify">
                    We further compare our method against two baselines: 1) a Reward Coder only baseline
                    where an LLM directly map user instructions to reward code instead of going through the Motion Descriptor, and 2) a Code-as-Policies baseline
                    where the LLM generates a plan for the robot motion using a set of pre-defined robot primitive skills instead of reward functions. 
                    For the Code-as-Policies (CaP) baseline, we design the primitive skills based on common commands available to the robot.
                    <br>
                    As shown below, our proposed approach achieves notably higher success rate for 11/17 task categories and comparable performance for the rest tasks,
                     showing the effectiveness and reliability of the proposed method.
                </p>
                <p style="text-align:center;">
                    <image width="90%" src="img/success_rate.png" class="img-responsive">
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
         
                <p class="text-justify">
                    We further showcase two examples where we teach the robot to perform complex tasks through multiple rounds of interactions.
                </p>
            </div>
        </div>

        <div class="row">  
            <div class="col-md-6">
            <p style="text-align:center;">
              <video width="100%" height="100%" controls autoplay loop muted>
                <source src="videos/sim/moonwalk_fixed_cam.mp4" type="video/mp4">
              </video>
              <p class="text-justify">
                Instruction 1: Make the robot stand upright on two back feet like a human. <br>
                Instruction 2: Good, you actually don't need to keep the front paws at certain height, just leave it to the controller. <br>
                Instruction 3: Good, now make the robot do a moonwalk. <br>
                Instruction 4: Moon walk means the robot should walk backward while the feet swings as if they are moving forward. Correct your answer.
            </p>
            </p>
            </div>
            <div class="col-md-6 ">
              <p style="text-align:center;">
              <video width="100%" height="100%" controls autoplay loop muted>
                <source src="videos/sim/apple_drawer.mp4" type="video/mp4">
              </video>
              <p class="text-justify">
                Instruction 1: Open the drawer. <br>
                Instruction 2: Good, now put the apple inside the drawer while keep it open. <br>
                Instruction 3: Good, now release the apple and move hand away. <br>
                Instruction 4: Now close the drawer.
              </p>
              </p>
            </div>   
      </div>

      <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <br>
            <h3>
                Validation on real hardware.
            </h3>
            <p class="text-justify">
                We implement a version of our method onto a mobile manipulator. 
                We detect objects in image-space using an open-vocabulary detector: F-VLM. 
                We extract the associated points from point cloud behind the mask and perform outlier 
                rejection for points that might belong to the background. From a birds-eye view, we fit 
                a minimum volume rectangle and take the extremes to determine the extent in the z-axis.
                We demonstrate sim-to-real transfer on two tasks: object pushing and object grasping. 
                Our system is able to generate relevant reward code and the Mujoco MPC is able to synthesize the pushing and grasping motion. 
            </p>
            <p style="text-align:center;">
                <video width="100%" height="100%" controls autoplay loop muted>
                  <source src="videos/real/l2r_real.mp4" type="video/mp4">
                </video>
            </p>

        </div>
    </div>
        

         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
TODO: add after arxiv version go live</textarea>
                </div>
            </div>
             
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The authors would like to acknowledge Ken Caluwaerts, Kristian Hartikainen, Steven Bohez, Carolina Parada, Marc Toussaint, and the greater teams at Google DeepMind for their feedback and contributions.             <br><br>
                The website template was borrowed from <a href="https://robotics-transformer.github.io/">RT-1</a>.
                </p>
            </div>
</div>
</div> 
</body>

<script>

    timeoutIds = [];
    
    function populateDemo(imgs) {
        // Get the expanded image
        var expandImg = document.getElementById("expandedImg");
        // Get the image text
        var imgText = document.getElementById("imgtext");
        var answer = document.getElementById("answer");
    
        // Use the same src in the expanded image as the image being clicked on from the grid
        expandImg.src = imgs.src.replace(".png", ".mp4");
        var video = document.getElementById('demo-video');
        // or video = $('.video-selector')[0];
        video.pause()
        video.load();
        video.play();

        console.log(expandImg.src);
        // Use the value of the alt attribute of the clickable image as text inside the expanded image
        var qa = imgs.alt.split("[sep]");
        imgText.innerHTML = qa[0];
        answer.innerHTML = "";
        // Show the container element (hidden with CSS)
        expandImg.parentElement.style.display = "block";
        for (timeoutId of timeoutIds) {
            clearTimeout(timeoutId);
        }
        typeWriter(qa[1], 0, qa[0]);
        }
    
    function typeWriter(txt, i, q) {
        var imgText = document.getElementById("imgtext");
        if (imgText.innerHTML == q) {
        if (i < txt.length) {
            if (txt.charAt(i) == "\\") {
                answer.innerHTML += "\n";
                i+=1;
            } else {
                answer.innerHTML += txt.charAt(i);
            }
            i++;
            timeoutIds.push(setTimeout(typeWriter, 5, txt, i, q));
        }
        else {
            hljs.highlightAll();
        }
        }
    }

    </script>

</html>
